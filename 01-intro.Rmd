# Introduction

Integrating artificial intelligence (AI) into educational settings has been debated for over three decades [@roll2016evolution]. Recent advances in generative AI and large language model (LLM) chatbots have opened even more possibilities for improvements in educational practices [@yan2024promises;@kasneci2023chatgpt;@tlili2023if;@debets2025chatbots]. Although LLMs mimic human intelligence, their applications extend beyond imitation and include a range of academic tasks: facilitating problem-solving [@urban2024chatgpt], providing personalised assistance [@darvishi2024impact], language learning [@derakhshan2024chatgpt;@song2023enhancing], evaluation of students' work [@henkel2024can]. However, the impact of these applications on students' learning remains controversial, with proponents highlighting potential benefits and critics cautioning about risks and drawbacks [@farrokhnia2024swot;@weidinger2022taxonomy;@walczak2023challenges]. 

A key advancement over previous AI systems is that LLMs can provide step-by-step reasoning to answer students' queries [@wei2022chain]. Although evidence suggests that such explanations can foster children's learning and contribute to the development of their scientific reasoning [@dejong1986explanation;@legare2014contributions;@danovitch2021mind], there is limited empirical research examining how such AI-generated explanations impact students' learning processes. Most prior studies have not isolated the specific role of detailed AI-generated reasoning in enhancing problem-solving skills. To fill this gap, we conducted a randomised experiment with k-12 students (n = 122) who were engaged in a simple estimation task -- guessing the value of coins in a jar. Half of the participants received only an AI-generated prediction, while the other half were additionally provided with AI-generated explanations detailing how to derive such estimation. This setup enabled to address our first research question (RQ): 

**RQ1: Do AI-generated explanations enhance students' problem-solving performance in school tasks, specifically in terms of estimation accuracy?** 

The distinction between AI-generated *explanations* and *solutions* further allows us to evaluate whether providing explanations influences students' trust and judgment of AI-generated content. Indeed, LLMs can generate inaccurate solutions and flawed reasoning [@saxton2019analysing;@kalyan2021how;@hendrycks2021measuring]. However, there is evidence that providing explanations increases users' trust and acceptance of AI systems, especially when the reasoning behind the output is clear [@rai2020explainable;@ferrario2022explainability]. In an education setting, this may affect learning outcomes in a complex manner. Students may interpret well-written explanations as evidence that an answer is correct, even when it is not. Conversely, they may notice minor errors or inconsistencies in an explanation that reduces their trust, even if the overall solution is accurate. This situation motivates our second research question:

**RQ2: How do AI-generated explanations influence students' perceived credibility of AI-generated solutions?**

Another critical technical progress of contemporary LLMs over previous AI systems is their ability to follow specific instructions when engaged in conversations.  Building on this capability, another goal of this study is to examine the impact of two fundamental modes of interaction on students' learning outcomes: one that fosters critical thinking through incremental guidance ("Socratic AI") and another that offers immediate solutions ("non-Socratic AI").

Although classical pedagogical methods like the Socratic Method have long been valued for promoting critical thinking and more profound understanding [@lam2011socratic;@dalim2022promoting;@wilberding2021socratic;@cleveland2015beyond], there is a notable gap in the AI education literature regarding how to integrate these methods into AI-driven solutions. Existing AI tools, like ChatGPT, often focus on providing direct solutions or feedback, potentially limiting opportunities for active student engagement and reflection. Our study addresses this gap by conducting a parallel experimental intervention comparing Socratic-style AI interactions to more conventional, solution-focused AI interactions in a K-12 setting. This comparison contributes novel evidence on how different AI interaction modes can influence student learning processes and outcomes, informing the design of more pedagogically sound AI educational systems.

More specifically, building on previous literature suggesting that the Socratic method encourages students to think critically and arrive at their answers rather than relying on the AI-generated solution, thus helping students gain more confidence in their reasoning [@cleveland2015beyond], we test the following research questions: 

**RQ3: Do student-AI interactions guided by the Socratic method promote better performance?** 

**RQ4: Do student-AI interactions guided by the Socratic method enhance students' confidence in their answers?** 

In addition to educational outcomes, exploring the trade-offs between Socratic and non-Socratic AI raises an important question about student *satisfaction*. If students find interacting with Socratic AI chatbots unengaging or frustrating, such perceptions might drive students away from Socratic AI to seek other (non-Socratic) tools. Indeed, research on conversation length with LLM-powered chatbots has shown that people tend to have mixed reactions when chatbots are instructed to engage in more extended conversations [@huang2024does]. Therefore, aligning educational benefits with user satisfaction is crucial for ensuring the long-term adoption of AI learning tools. This consideration leads to our last research question:

**RQ5: How do students perceive the helpfulness of Socratic AI compared to non-Socratic AI?**

<!-- In this study, we use a randomised experiment to investigate these questions. For  by collecting detailed information about students' conversation logs and length with LLM-based chatbot, which we combine with self-reported data -->

Addressing these questions aims to contribute to a growing debate on using pedagogical principles, like a Socratic approach, to integrate AI in education, which has recently gained considerable attention. For instance, Khan Academy has developed its AI tutor, Khanmigo, based on Socratic principles, aiming to foster critical thinking and engagement in learners.^[https://docsbot.ai/prompts/education/khanmigo-lite-socratic-tutor] Several commentators have argued that the Socratic method is particularly effective for designing AI tutors for children, as it enhances critical thinking and discourages students from copying AI-generated responses without scrutiny [@lara2020artificial].^[See also https://research.gatech.edu/ai-oral-assessment-tool-uses-socratic-method-test-students-knowledge] Beyond education, Socratic AI systems are also being developed to foster critical thinking among the general public, particularly in efforts to combat disinformation [@duelen2024socratic].

## Literature Review

A growing body of research highlights the tremendous potential of LLMs to improve student learning outcomes. A recent scoping review identifies 53 distinct use cases for LLMs in education [@yan2024practical], ranging from automatic grading and personalised feedback to teaching support and content generation, reflecting the high versatility and scalability of LLMs. Within this expanding literature, this study centers on *AI chatbots*, conversational agents powered by LLMs, and their educational applications, reflecting their increasing prominence and potential impact in this field.

Early investigations into the use of AI chatbots, such as ChatGPT, in educational settings have identified both opportunities and challenges [@farrokhnia2024swot;@noroozi2024generative]. On one hand, ChatGPT can serve as a personalised tutor, assist in drafting assignments, or generate creative prompts for classroom activities. On the other hand, concerns have been raised around academic integrity, overreliance on AI-generated content, and the need for critical thinking skills when interpreting chatbot outputs. 

A recent review of over 70 studies offers a broad look at how chatbots are used in different areas of education [@debets2025chatbots]. It groups chatbot uses by roles like tutoring, giving feedback, and helping with administrative tasks, and examines how they are designed and used. The review finds that AI chatbots generally meet their goals. For example, a quasi-experimental study with 320 middle schoolers found that an AI teaching chatbot significantly improved math knowledge [@xing2025development]. Similarly, a meta-analysis of 24 RCTs shows that AI chatbots significantly enhance student outcomes, with stronger effects in higher education [@wu2024ai]. However, most studies are short-term, may suffer from publication bias, and often lack a strong theoretical foundation. 

Despite promising results, key limitations remain. For instance, @er2024assessing found that students in a programming class performed better with instructor feedback, which was seen as more useful and fair than AI-generated feedback, highlighting the current gap between AI and human instruction. Simiarly, evidence of performance boosts from non-education settings is mixed. A meta-analysis by @vaccaro2024combinations found that human-AI teams often performed worse than either humans or AI alone, suggesting a more nuanced relationship. In education, additional concerns are that intagrating AI might encourage cheating, over-reliance, or superficial understanding [@farrokhnia2024swot;@yusuf2024generative;@eke2023chatgpt;@lee2024cheating], the risk of which is hard to evaluate without longitudinal studies.

The effectiveness of AI in education often depends on students' attitudes and perceived usefulness of AI tools, understanding which can guide curriculum design. For instance, @li2024explanatory found that K–12 students’ views on AI tools were linked to factors like readiness, confidence, and anxiety. Similar concerns have been noted from teachers' perspectives as well [@kaplan2023generative]. Our study builds on this by exploring how interaction styles, such as Socratic vs. non-Socratic, shape students' perceptions of AI helpfulness.

This study advances this literature on human-AI collaboration in educational contexts by examining the role of step-by-step reasoning and the impact of different modes of AI interaction. Additionally, it contributes to ongoing research exploring the factors that influence students' engagement with AI. Gaining insights into the value these combinations bring is crucial for developing practical guidelines for schools and educational institutions.

Most existing research focuses on general AI chatbots, like ChatGPT, which are not specifically designed for students and often lack clear pedagogical foundations. Our study highlights the importance of purposeful design and pedagogical goals, contributing to the growing field of AI chatbot design for education, or design for learning [@gavsevic2023empowering]. This research calls for a multidisciplinary approach combining pedagogy and human-computer interaction. Building on recent efforts [@dai2023can; @yan2024practical; @joksimovic2023opportunities], we examine how K–12 students interact with different AI chatbot styles. By doing so, thee study underscores both the potential and challenges of applying classical pedagical methods, such as the Socratic approach, emphasizing the need to provide explanations rather than just answers.