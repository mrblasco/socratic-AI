# Introduction

Integrating artificial intelligence (AI) into educational settings has been debated for over three decades [@roll2016evolution]. Recent advances in generative AI and large language model (LLM) chatbots have opened even more possibilities for improvements in educational practices [@yan2024promises;@kasneci2023chatgpt;@tlili2023if;@debets2025chatbots]. Although LLMs mimic human intelligence, their applications extend beyond imitation and include a range of educational tasks: facilitating problem-solving [@urban2024chatgpt], language learning [@derakhshan2024chatgpt;@song2023enhancing], or evaluation of students' work [@henkel2024can]. However, the impact of these applications on students' learning remains controversial, with proponents highlighting potential benefits and critics cautioning about risks and drawbacks [@farrokhnia2024swot;@weidinger2022taxonomy;@walczak2023challenges]. This study addresses two critical aspects of AI integration in education: the effectiveness of AI-generated explanations and the optimal modes of interaction between AI and students, specifically whether Socratic or alternative approaches yield better outcomes.

A key advancement over previous AI systems is that LLMs can generate step-by-step solutions to accompany their responses to students' queries. This study investigates how this ability influences students' interactions and helps them enhance their problem-solving skills. Specifically, we first examine the impact of AI explanations on students' performance in a numerical estimation task. We then test whether providing such AI explanations influence students' assessment of the accuracy of AI-generated predictions, compared to presenting the predictions alone. Finally, we test two approaches throught which students can interact with AI chatbots and receive explanations: one approach that encourages critical thinking through incremental guidance ("Socratic AI")  and another providing immediate solutions ("non-Socratic AI").

<!-- A key advancement over previous AI systems is that LLMs can generate step-by-step solutions to accompany their responses to students' queries. This study investigates how LLMs' ability to generate step-by-step explanations influences students' problem-solving skills and their critical evaluation of AI-generated responses. Specifically, we examine the impact of AI explanations on students' performance in a numerical estimation task and their assessment of the accuracy of AI-generated predictions. -->

Previous research has shown that explanations can foster children's causal learning and contribute to the development of their scientific reasoning [@dejong1986explanation;@legare2014contributions; @danovitch2021mind]. Similarly,  explanations generated by LLMs may guide students in their problem-solving, for example, by decomposing a numerical estimation task into more manageable steps, such as considering heuristics or approximations. However, LLMs can also generate inaccuracies and flawed reasoning [@saxton2019analysing; @kalyan2021how; @hendrycks2021measuring], misleading students if followed without scrutiny. Understanding this dual role of AI explanations as a valuable educational tool and a potential source of misinformation leads to our first research questions (RQs):

**RQ1: Do AI-generated explanations enhance students' problem-solving performance in school tasks?** 

Research in AI shows that providing explanations can increase users' trust and acceptance of AI systems, especially when the reasoning behind the output is clear [@rai2020explainable;@ferrario2022explainability]. However, in educational settings, trust is more complex. Students may interpret well-written explanations as evidence that an answer is correct, even when it is not. Conversely, they may notice minor errors or inconsistencies in an explanation that reduce their trust, even if the overall solution is accurate. This motivates our second research question:

**RQ2: How do AI-generated explanations influence students’ perceived credibility of AI-generated solutions?**

The study also aims to explore how different modes of student-AI interaction more broadly influence students' performance and perceptions. Specifically, we contrast two fundamental approaches: the "Socratic” and “non-Socratic” methods. The Socratic approach, inspired by the Socratic teaching method [@lam2011socratic;@dalim2022promoting], engages students through an argumentative dialogue where the AI tutor asks thought-provoking questions to stimulate critical thinking and self-reflection [@wilberding2021socratic]. This method encourages students to think critically and arrive at their answers rather than relying on the AI-generated solution, thus helping students gain more confidence in their reasoning [@cleveland2015beyond]. In contrast, the non-Socratic approach directly answers students' queries without necessarily engaging them in a dialogue or exploration.

The Socratic approach has recently gained considerable attention in the context of AI tutoring. Notably, Khan Academy has developed its AI tutor, Khanmigo, based on these principles, aiming to foster critical thinking and engagement in learners.^[https://docsbot.ai/prompts/education/khanmigo-lite-socratic-tutor]. Several commentators have argued that the Socratic method is particularly effective for designing AI tutors for children, as it enhances critical thinking and discourages students from copying AI-generated responses without scrutiny [@lara2020artificial].^[See also https://research.gatech.edu/ai-oral-assessment-tool-uses-socratic-method-test-students-knowledge] Beyond education, Socratic AI systems are also being developed to foster critical thinking among the general public, particularly in efforts to combat disinformation [@duelen2024socratic]. This debate raises the following research questions:

**RQ3: Do student-AI interactions guided by the Socratic method promote deeper critical thinking?** 

**RQ4: Do student-AI interactions guided by the Socratic method enhance students' confidence in their answers?** 

A key challenge when choosing between Socratic and non-Socratic AI is assessing the extent of "user satisfaction" among students, regardless of its academic benefits. If students find interacting with Socratic AI chatbots unengaging or frustrating, such perceptions might drive students away from Socratic AI and seek (non-Socratic) tools that provide direct answers. Indeed, research on conversation length with LLM-powered Chatbots
has shown that people tend to have mixed reactions when chatbots are instructed to engage in longer conversations [@huang2024does]. Therefore, aligning educational benefits with user satisfaction is crucial for ensuring the long-term adoption of AI learning tools. This leads to our last research question:

**RQ5: How do students perceive the helpfulness of Socratic AI compared to non-Socratic AI?**

By focusing on the impact of AI explanations and the Socratic AI approach on students' critical thinking, we aim to contribute to a growing literature about integrating AI into education. Recent research on AI chatbots and LLMs indicates significant potential for improving student learning outcomes. A meta-analysis of 24 randomised studies highlights the positive impact of chatbots, especially on student productivity and learning engagement [@wu2024ai]. However, several studies are more cautious about the benefits, partly due to concerns that AI might encourage cheating, over-reliance, or superficial understanding, which are challenging to measure in short-term studies [@lee2024cheating]. Additional studies on LLMs' impact on academic integrity show that students and teachers struggle to differentiate between human and AI-generated text, raising risks of increased academic dishonesty and eroded trust [@yusuf2024generative; @eke2023chatgpt]. Furthermore, as LLMs continue to evolve, students and educators may lack the skills to leverage them effectively, which could result in missed educational opportunities [@kaplan2023generative].

<!-- The ethical and societal implications of LLMs in education are also complex. Issues such as bias, privacy, surveillance, and autonomy persist, especially in K-12 contexts, where the impact of algorithmic decisions on young learners' agency and privacy can be profound [@williams2024ethical]. LLMs introduce unique challenges, notably in alignment, where the system's goals may not align fully with educational objectives, despite companies' efforts to mitigate such risks through fine-tuning and reinforcement learning [@gabriel2020artificial]. Concerns also extend to students' digital well-being, as AI in education could inadvertently contribute to unhealthy digital habits and reliance on AI assistance over critical thinking [@weidinger2022taxonomy]. Given these multidimensional impacts, further research is needed to optimise AI's role in education, balancing short-term learning gains with long-term skill development and ethical considerations. -->

<!-- Contributions -->

Research in the field of human-AI combinations does not indicate consensus in terms of its effectiveness. For example, using a meta-analysis of studies published between 2020 and 2023, @vaccaro2024combinations finds that, on average, human-AI combinations performed significantly worse than the best of humans or AI alone. On the other hand, @li2024explanatory uses a sample of 200 Chinese students to show significant correlations between human–computer interactions, perceived usefulness and students' skills. Understanding these associations can help institutions design better AI courses and more effective integration of AI in education. 

This study advances this literature on human-AI collaboration in educational contexts by examining the role of step-by-step reasoning and the impact of different modes of AI interaction. Additionally, it contributes to ongoing research exploring the factors that influence students' engagement with AI. Gaining insights into the value these combinations bring is crucial for developing practical guidelines for schools and educational institutions.

Results of our investigation also contribute to the current debate on the pedagogical principles for designing AI-driven systems and educational opportunities, the so-called _design for learning_ [@gavsevic2023empowering]. It underscores the importance of using a multi-disciplinary approach that combines traditional pedagogical insights with principles from human-computer interactions. Building on recent efforts to address critical challenges in integrating AI into education ---like providing feedback, grading, and making learning recommendations [@dai2023can; @yan2024practical; @joksimovic2023opportunities]--- this study offers insights on how k-12 students engage with alternative forms of AI interaction. In particular, it emphasizes the challenges of embedding classical pedagogical principles, such as the Socratic Method, and the value of providing AI explanations for students compared to just solutions.

<!-- One concern is that students will rely on AI results instead of learning from it, resulting in an ineffective AI human combination. In a related experimental study, @darvishi2024impact (n = 1625)  -->

