# Introduction

Integrating artificial intelligence (AI) into educational settings has been debated for over three decades [@roll2016evolution]. Recent advances in generative AI and large language model (LLM) chatbots have opened even more possibilities for improvements in educational practices [@yan2024promises; @kasneci2023chatgpt; @tlili2023if]. Although LLMs mimic human intelligence, their applications extend beyond imitation and include a range of educational tasks: facilitating problem-solving [@urban2024chatgpt], language learning [@derakhshan2024chatgpt; @song2023enhancing], or evaluation of students' work [@henkel2024can]. However, the impact of these applications on students' learning remains controversial, with proponents highlighting potential benefits and critics cautioning about risks and drawbacks. This study addresses two critical aspects of AI integration in education: the effectiveness of AI-generated explanations and the optimal modes of interaction between AI and students, specifically whether Socratic or alternative approaches yield better outcomes.


A key advancement over previous AI systems is that LLMs can generate step-by-step solutions to accompany their responses to students' queries. This study investigates how this ability influences students' interactions and helps them enhance their problem-solving skills. Specifically, we first examine the impact of AI explanations on students' performance in a numerical estimation task and their assessment of the accuracy of AI-generated predictions. Then, we test two approaches to access these explanations through AI chatbots: one approach that encourages critical thinking through incremental guidance ("Socratic AI")  and another providing immediate solutions ("non-Socratic AI").


<!-- A key advancement over previous AI systems is that LLMs can generate step-by-step solutions to accompany their responses to students' queries. This study investigates how LLMs' ability to generate step-by-step explanations influences students' problem-solving skills and their critical evaluation of AI-generated responses. Specifically, we examine the impact of AI explanations on students' performance in a numerical estimation task and their assessment of the accuracy of AI-generated predictions. -->

Previous research has shown that explanations can foster children’s causal learning and contribute to the development of their scientific reasoning [@dejong1986explanation;@legare2014contributions; @danovitch2021mind]. Similarly,  explanations generated by LLMs may guide students in their problem-solving, for example, by decomposing a numerical estimation task into more manageable steps, such as considering heuristics or approximations. However, LLMs can also generate inaccuracies and flawed reasoning [@saxton2019analysing; @kalyan2021how; @hendrycks2021measuring], misleading students if followed without scrutiny. Understanding this dual role of AI explanations as a valuable educational tool and a potential source of misinformation leads to our first research question (RQ):

**RQ-1: Do AI-generated explanations enhance learning by providing insights, or do they risk impairing students' critical thinking by presenting inaccurate reasoning as authoritative?**

The study also aims to explore how different modes of student-AI interaction more broadly influence students' performance and perceptions, of which providing explanations is one key element. Specifically, we contrast two fundamental approaches: the "Socratic” and “non-Socratic” methods. The Socratic approach, inspired by the Socratic teaching method [@lam2011socratic; @dalim2022promoting], engages students through an argumentative dialogue where the AI tutor asks thought-provoking questions to stimulate critical thinking and self-reflection [@wilberding2021socratic]. This method encourages students to think critically and arrive at their answers rather than relying on the AI-generated solution, thus helping students gain more confidence in their reasoning [@cleveland2015beyond]. In contrast, the non-Socratic approach directly answers students' queries without necessarily engaging them in a dialogue or exploration.

The Socratic approach has recently gained considerable attention in the context of AI tutoring in education. Notably, Khan Academy has developed its AI tutor, Khanmigo, based on these principles, aiming to foster critical thinking and engagement in learners.^[https://docsbot.ai/prompts/education/khanmigo-lite-socratic-tutor]. Several commentators have argued that the Socratic method is particularly effective for designing AI tutors for children, as it enhances critical thinking and discourages students from copying AI-generated responses without scrutiny [@lara2020artificial].^[See also https://research.gatech.edu/ai-oral-assessment-tool-uses-socratic-method-test-students-knowledge] This debate raises the following research question:

**RQ-2: Do student-AI interactions guided by the Socratic method promote deeper critical thinking and enhance students' confidence in their answers?** 

Another objective of comparing Socratic and non-Socratic AI is to assess the extent of "user satisfaction" among students. Regardless of its academic benefits, Socratic AI could find limited application if students find interacting with Socratic AI chatbots unengaging or frustrating. Negative interactions or perceptions might drive students away in favour of more straightforward (non-Socratic) tools that provide direct answers, such as ChatGPT. Therefore, aligning educational benefits with user satisfaction is crucial for ensuring the long-term adoption of AI learning tools. This leads to our last research question:

**RQ-3: How do students perceive the helpfulness of Socratic AI compared to non-Socratic AI?**

By focusing on the impact of AI explanations and the Socratic AI approach on students' critical thinking, we aim to contribute to a growing literature about integrating AI into education. Recent research on AI chatbots and LLMs indicates significant potential for improving student learning outcomes. A meta-analysis of 24 randomised studies highlights the positive impact of chatbots, especially on student productivity and learning engagement [@wu2024ai]. However, several studies are more cautious about the benefits, partly due to concerns that AI might encourage cheating, over-reliance, or superficial understanding, which are challenging to measure in short-term studies [@lee2024cheating]. Additional studies on LLMs' impact on academic integrity show that students and teachers struggle to differentiate between human and AI-generated text, raising risks of increased academic dishonesty and eroded trust [@yusuf2024generative; @eke2023chatgpt]. Furthermore, as LLMs continue to evolve, students and educators may lack the skills to leverage them effectively, which could result in missed educational opportunities [@kaplan2023generative].

The ethical and societal implications of LLMs in education are also complex. Issues such as bias, privacy, surveillance, and autonomy persist, especially in K-12 contexts, where the impact of algorithmic decisions on young learners' agency and privacy can be profound [@williams2024ethical]. LLMs introduce unique challenges, notably in alignment, where the system's goals may not align fully with educational objectives, despite companies' efforts to mitigate such risks through fine-tuning and reinforcement learning [@gabriel2020artificial]. Concerns also extend to students' digital well-being, as AI in education could inadvertently contribute to unhealthy digital habits and reliance on AI assistance over critical thinking [@weidinger2022taxonomy]. Given these multidimensional impacts, further research is needed to optimise AI's role in education, balancing short-term learning gains with long-term skill development and ethical considerations.

<!-- Contributions -->

Research in the field of human-AI combinations does not indicate consensus in terms of its effectiveness. For example, using a meta-analysis of studies published between 2020 and 2023, @vaccaro2024combinations finds that, on average, human-AI combinations performed significantly worse than the best of humans or AI alone. On the other hand, @li2024explanatory uses a sample of 200 Chinese students to show significant correlations between human–computer interactions, perceived usefulness and students' skills. Understanding these associations can help institutions design better AI courses and more effective integration of AI in education. 

This study advances this literature on human-AI collaboration in educational contexts by examining the role of step-by-step reasoning and the impact of different modes of AI interaction. Additionally, it contributes to ongoing research exploring the factors that influence students' engagement with AI. Gaining insights into the value these combinations bring is crucial for developing practical guidelines for schools and educational institutions.

Results of our investigation also contribute to the current debate on the pedagogical principles for designing AI-driven systems and educational opportunities, the so-called _design for learning_ [@gavsevic2023empowering]. It underscores the importance of using a multi-disciplinary approach that combines traditional pedagogical insights with principles from human-computer interactions. Consequently, our work adds to the growing efforts to tackle critical challenges in integrating AI into education, helping to automate and scale up tasks like providing feedback, grading, and making learning recommendations [@dai2023can; @yan2024practical; @joksimovic2023opportunities].


<!-- One concern is that students will rely on AI results instead of learning from it, resulting in an ineffective AI human combination. In a related experimental study, @darvishi2024impact (n = 1625)  -->

