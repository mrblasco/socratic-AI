# Highlights {-}

- An experiment was conducted where K-12 students (N=122) used AI chatbots to solve school-related problems. 

- Two interventions were tested: (1) comparing AI-generated step-by-step explanations to predictions alone, and (2) contrasting AI chatbot support inspired by the Socratic method (guided questioning) versus  non-Socratic (direct answers) support.

<!-- - Results showed that AI-generated explanations improved students' performance over solutions without them, highlighting the value of AI-generated guidance. -->
 
- Participants performed better under step-by-step explanations. They also engaged more frequently with the Socratic AI, though this approach did not result in improved performance and retain of knowledge compared to the non-Socratic AI. 

- While students found interactions with the AI useful, they perceived the Socratic AI as less helpful than non-Socratic overall.

- The findings highlight challenges in designing AI tutors that effectively foster critical thinking while maintaining student satisfaction, raising concerns about their adoption in educational settings. 

# Abstract {-}

How does integrating large language models (LLMs) into classroom activities influence students' learning? To investigate this question, we conducted a randomized experiment with students aged 14-16 (N=122), testing two interventions: (i) comparing AI-generated solutions with step-by-step explanations versus solutions alone and (ii) Socratic AI chatbots promoting critical thinking through incremental guidance versus non-Socratic AI offering direct solutions. Students completed school-related tasks under these conditions. We collected students' performance and interaction logs data, as well as their attitudes through self-reported surveys. Results indicated that AI-generated explanations significantly improved students' performance over solutions alone, highlighting the benefits of step-by-step guidance. The Socratic AI approach fostered significantly greater engagement and interaction. However, it did not achieve significant improvements in learning, and a higher fraction of students perceived it as less helpful. Furthermore, despite overall positive perceptions of AI assistance, students exhibited limited retention, failing to apply learned concepts to new situations without the aid of AI. These findings underscore both the potential and limitations of LLM-based chatbots in K-12 education, especially in balancing the development of critical thinking with user satisfaction.


<!-- How does integrating Large Language Models (LLMs) into classroom activities influence students' learning? We conduct a randomised experiment to examine whether LLMs' interactions and step-by-step reasoning improve students' performance. Specifically, we compare chatbots designed to encourage critical thinking through incremental guidance ("Socratic AI") against chatbots providing immediate solutions ("non-Socratic AI"), evaluating the effects on students' engagement, interactions, and learning outcomes. Students aged 14 to 16 participated in problem-solving and writing tasks under different experimental conditions. We evaluated their performance through conversation logs and test scores on various school-related tasks. Attitudes were measured using self-reported surveys.  -->


**Keywords**: artificial intelligence; large language models; learning; education policy; experiment; k-12


<!-- Competing Interests: Authors are required to disclose financial or non-financial interests that are directly or indirectly related to the work submitted for publication. Please refer to “Competing Interests and Funding” below for more information on how to complete this section. -->

<!-- 
Cite references in the text by name and year in parentheses. Some examples:

Negotiation research spans many disciplines (Thompson, 1990).
This result was later contradicted by Becker and Seligman (1996).
This effect has been widely studied (Abbott, 1991; Barakat et al., 1995; Kelso & Smith, 1998; Medvec et al., 1999). 
-->