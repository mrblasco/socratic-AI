# Discussion 

The present study aims to contribute to our understanding of the impact of pedagogically aligned configurations of an LLM-based tool on high-school students’ learning and attitudes with a special focus on their critical thinking. 

## Overview of the findings

With the rapid adoption of AI in education, the focus on students' competencies and skills, such as critical thinking, has become particularly urgent [@walter2024embracing;@holmes2023guidance]. Despite the rapidly increasing body of research on the impact of LLMs on students’ learning, most studies focus on participants in higher education [e.g.,@suriano2025student], which do not necessarily generate insights that apply to younger students who have different cognitive maturity. In addition, despite the increasing research interest regarding the impact of LLMs on students’ learning, there is little consensus among researchers, especially due to students’ over-reliance on AI tools [@zhai2024effects].

<!-- as individuals increasingly favor fast and optimal solutions over slow ones constrained by practicality -->

The results of this study focus on the role of two interventions on the configuration of an LLM chatbot for educational support: (i) step-by-step explanations and (ii) the Socratic method with guiding questions. First, we found that students significantly benefit from AI-generated step-by-step reasoning accompanying solutions, mainly when performing open-ended tasks like estimating unknown quantities. This finding aligns with expectations of current research on the use of LLM chatbots and their ability to provide explanations for learning purposes [@wu2024critical]. It also aligns with previous research demonstrating that LLMs can enhance student performance [@xing2025development]. However, our results underscore that the key mechanism driving AI improvements is the step-by-step reasoning provided by the AI, which allows students to understand better and engage with problem-solving. 

<!-- This insight suggests that teachers should focus on educating students to enhance their ability to evaluate and judge the correctness of AI-generated reasoning. -->

Furthermore, our study reveals that step-by-step reasoning not only helps students in solving problems but also enhances students' ability to evaluate AI-generated information critically. Critical thinking is indeed a complex cognitive process that involves evaluation and analysis of a given information [@lai2011critical]. The mobilisation of students' critical thinking in the specific task was evidenced by the more positive evaluations of human-generated guesses compared to AI-generated solutions, suggesting that students could better assess and challenge AI predictions. This contribution extends the existing literature by showing that AI can foster critical thinking and analytical skills when coupled with transparent reasoning, instead of being presented  as a "black box" [@bearman2023learning].

In addition, we compared various ways to structure student-AI interactions and, more specifically, the effectiveness of Socratic AI (an interactive, questioning-based AI) with non-Socratic AI. Our results showed that Socratic AI was more engaging and promoted greater interaction, aligning with the notion that AI-student interactions should be dynamic and dialogical [@suriano2025student]. However, contrary to our expectations, we found no significant differences in students' self-reported confidence in the accuracy of their answers or in the correctness of their responses despite the more frequent interactions with the Socratic AI. Additionally, the Socratic AI's perceived helpfulness was rated lower compared to the non-Socratic AI. These results cast some doubts on the effectiveness of Socratic AI in short-term tasks or, more broadly, the effect of certain kinds of AI-student interactions. As such, existing pedagogical practices extensively used in human-human interaction might not always work in human-AI interaction. In addition, while recent research attempts to understand the quality of Socratic LLMs for critical thinking [e.g.,@favero2024enhancing], these studies use simulations without experiments with human participants. Our results underscore the need for adapting existing pedagogical paradigms or proposing new ones for integrating LLM tools into pedagogical practices effectively.

Our findings also indicate that simply interacting with AI cannot promote meaningful and lasting learning. In our initial test on sound propagation, approximately half of the students could revise their initial answers based on AI interactions, improving their response accuracy from 30% to 70%. However, in a verification task where students had no access to AI, the majority failed to identify the correct answer, mistakenly claiming that sound propagates faster in water than in gold. Most students exhibited this misunderstanding, which supports key concerns that AI-generated answers alone do not scaffold effective learning, and the mere implementation of Socratic AI does not mitigate this risk.

Our results suggest that AI has great potential as an educational tool, but its implementation requires careful considerations. First, students with prior experience in using AI tools may not readily adopt new pedagogical approaches, especially if they perceive them as less effective than commercially available alternatives. Second, the effectiveness of the pedagogical approach may vary depending on the nature of the task, complicating the design of a one-size-fits-all solution for AI-assisted learning.

## Limitations of the study and future work

Several limitations of our study should be acknowledged. The small sample size limits the generalizability of our findings, though the controlled environment and the use of multiple tasks help mitigate potential noise in the data. Also, our experiment focused on short-term results. Although short term results are important to foster adoption, it is unclear the effectiveness of AI in the long-term. Another limitation is that learning retention was tested using only one specific physics question. Although we controlled for prior knowledge and carefully designed a simple task to fit within a 40-minute intervention, additional questions would be needed to rule out confounding factors. Additionally, our study combined objective performance metrics with self-assessed ratings of helpfulness and confidence. However, individual perceptions do not necessarily reflect actual learning [@noroozi2025does]. Further research is needed to validate our findings using objective learning measures.

Finally, we conducted our study with a cohort of students possessing strong English proficiency and a clear understanding of the limitations of AI, which may not be representative of the general student population. Additionally, we focused on only two schools, which allowed us to control for school-specific fixed effects. However, we recognize that the impact of the treatments may differ across schools, and a broader investigation involving many more institutions would be necessary to explore this variability. Furthermore, the experiment was carried out at school and in a secure and anonymous digital environment, with a robust protocol developed to ensure the safe and ethical handling of AI-based interactions in experimental settings. However, it remains to be seen if the results of our analysis will remain when students use AI in the field.  

Our future work includes a large-scale study with a representative sample in a country in Europe, where we aim to address the above-mentioned limitations.

## Concluding remarks

While our study focused on comparing a fairly general pedagogical approach---Socratic AI, future research should explore this pedagogical method in a more nuanced way and consider alternative pedagogical approaches to facilitate scalability of AI tools across diverse tasks and educational contexts. Yet, our findings have important implications for designing AI tutors, highlighting the importance of  providing transparent AI-generated step-by-step reasoning and the challenges of fostering learning through guided AI-student interactions. Therefore, AI systems must engage students interactively and provide learning opportunities for students' critical thinking and problem-solving skills to maximise their educational value. Future developments should focus on refining the integration of AI-generated reasoning and ensuring that AI tools are adaptable to various learning tasks and students' needs.
